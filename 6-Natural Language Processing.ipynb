{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing \n",
    "\n",
    "Can we make computers understand words and sentences? As mentioned in the previous chapter, one of the goals is to match or surpass important human capabilities. One of those capabilities is language (communication, knowing the meaning of something, arriving at conclusions based on the words and sentences). \n",
    "\n",
    "This is where Natural Language Processing or NLP comes in. It’s a branch of artificial intelligence wherein the focus is on understanding and interpreting human language. It can cover the understanding and interpretation of both text and speech. \n",
    "\n",
    "Have you ever done a voice search in Google? Are you familiar with chatbots (they automatically respond based on your inquiries and words)? What about Google Translate?\n",
    "\n",
    "It’s Natural Language Processing (NLP) at work. In fact, within a few or several years the NLP market might become a multi-billion dollar industry. That’s because it could be widely used in customer service, creation of virtual assistants (similar to Iron Man’s JARVIS), healthcare documentation, and other fields. \n",
    "\n",
    "Natural Language Processing is even used in understanding the content and gauging sentiments found in social media posts, blog comments, product reviews, news, and other online sources. NLP is very useful in these areas due to the massive availability of data from online activities. Remember that we can vastly improve our data analysis and machine learning model if we have sufficient amounts of quality data to work on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Words & Sentiments \n",
    "\n",
    "\n",
    "One of the most common uses of NLP is in understanding the sentiment in a piece of text (e.g. Is it a positive or negative product review?What does the tweet say overall?). If we only have a dozen comments and reviews to read, we don’t need any technology to do the task. But what if we have to deal with hundreds or thousands of sentences to read? \n",
    "\n",
    "Technology is very useful in this large-scale task. Implementing NLP can make our lives a bit easier and even make the results a bit more consistent and reproducible. \n",
    "To get started, let’s study Restaurant_Reviews.tsv (let’s take a peek): \n",
    "\n",
    "Wow... Loved this place.1\n",
    "Crust is not good.0\n",
    "Not tasty and the texture was just nasty.0\n",
    "Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.              1\n",
    "The selection on the menu was great and so were the prices.1\n",
    "Now I am getting angry and I want my damn pho.0\n",
    "Honeslty it didn't taste THAT fresh.)0\n",
    "The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.              0\n",
    "The fries were great too.1 \n",
    "\n",
    "The first part is the statement wherein a person shares his/her impression or experience about the restaurant. The second part is whether that statement is negative or not (0 if negative, 1 if positive or Liked). Notice that this is very similar with Supervised Learning wherein there are labels early on. \n",
    "\n",
    "However, NLP is different because we’re dealing mainly with text and language instead of numerical data. Also, understanding text (e.g. finding patterns and inferring rules) can be a huge challenge. That’s because language is often inconsistent with no explicit rules. For instance, the meaning of the sentence can change dramatically by rearranging, omitting, or adding a few words in it. There’s also the thing about context wherein how the words are used greatly affect the meaning. We also have to deal with “filler” words that are only there to complete the sentence but not important when it comes to meaning. \n",
    "\n",
    "Understanding statements, getting the meaning and determining the emotional state of the writer could be a huge challenge. That’s why it’s really difficult even for experienced programmers to come up with a solution on how to deal with words and language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK \n",
    "\n",
    "\n",
    "One of the most popular suites is the Natural Language Toolkit (NLTK). \n",
    "With NLTK (developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.), text processing becomes a bit more straightforward because you’ll be implementing pre-built code instead of writing everything from scratch. In fact, many countries and universities actually incorporate NLTK in their courses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import dataset with setting delimiter as ‘\\t’ as columns are separated as tab space. \n",
    "Reviews and their category(0 or 1) are not separated by any other symbol but with tab space as most of the other symbols are is the review (like $ for price, ….!, etc) and the algorithm might use them as delimiter, which will lead to strange behavior (like errors, weird output) in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries \n",
    "import numpy as np   \n",
    "import pandas as pd  \n",
    "  \n",
    "# Import dataset \n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Text Cleaning or Preprocessing\n",
    "Remove Punctuations, Numbers: Punctuations, Numbers doesn’t help much in processong the given text, if included, they will just increase the size of bag of words that we will create as last step and decrase the efficency of algorithm.\n",
    "\n",
    "\n",
    "Stemming: Take roots of the word\n",
    " \n",
    "Convert each word into its lower case: For example, it useless to have same words in different cases (eg ‘good’ and ‘GOOD’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bradl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# library to clean data \n",
    "import re    \n",
    "# Natural Language Tool Kit \n",
    "import nltk    \n",
    "nltk.download('stopwords')  \n",
    "# to remove stopword \n",
    "from nltk.corpus import stopwords  \n",
    "# for Stemming propose  \n",
    "from nltk.stem.porter import PorterStemmer   \n",
    "# Initialize empty array \n",
    "# to append clean text  \n",
    "corpus = []    \n",
    "# 1000 (reviews) rows to clean \n",
    "for i in range(0, 1000):        \n",
    "    # column : \"Review\", row ith \n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])        \n",
    "    # convert all cases to lower cases \n",
    "    review = review.lower()        \n",
    "    # split to array(default delimiter is \" \") \n",
    "    review = review.split()        \n",
    "    # creating PorterStemmer object to \n",
    "    # take main stem of each word \n",
    "    ps = PorterStemmer()        \n",
    "    # loop for stemming each word \n",
    "    # in string array at ith row     \n",
    "    review = [ps.stem(word) for word in review \n",
    "                if not word in set(stopwords.words('english'))]                    \n",
    "    # rejoin all string array elements \n",
    "    # to create back into a string \n",
    "    review = ' '.join(review)         \n",
    "    # append each string to create \n",
    "    # array of clean text  \n",
    "    corpus.append(review)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Tokenization\n",
    "involves splitting sentences and words from the body of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Making the bag of words via sparse matrix\n",
    "Take all the different words of reviews in the dataset without repeating of words.\n",
    "\n",
    "\n",
    "One column for each word, therefore there are going to be many columns.\n",
    "\n",
    "\n",
    "Rows are reviews\n",
    "\n",
    "\n",
    "If word is there in row of dataset of reviews, then the count of word will be there in row of bag of words under the column of the word.\n",
    "\n",
    "For this purpose we need CountVectorizer class from sklearn.feature_extraction.text.\n",
    "We can also set max number of features (max no. features which help the most via attribute “max_features”). Do the training on corpus and then apply the same transformation to the corpus “.fit_transform(corpus)” and then convert it into array. If review is positive or negative that answer is in second column of : dataset[:, 1] : all rows ans 1st column (indexing from zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "  \n",
    "# To extract max 1500 feature. \n",
    "# \"max_features\" is attribute to \n",
    "# experiment with to get better results \n",
    "cv = CountVectorizer(max_features = 1500)  \n",
    "  \n",
    "# X contains corpus (dependent variable) \n",
    "X = cv.fit_transform(corpus).toarray()  \n",
    "  \n",
    "# y contains answers if review \n",
    "# is positive or negative \n",
    "y = dataset.iloc[:, 1].values  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of the dataset to be used:\n",
    "*Columns seperated by \\t (tab space)*\n",
    "\n",
    "\n",
    "*First column is about reviews of people*\n",
    "\n",
    "\n",
    "*In second column, 0 is for negative review and 1 is for positive review*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Splitting Corpus into Training and Test set. \n",
    "For this we need class train_test_split from sklearn.cross_validation. Split can be made 70/30 or 80/20 or 85/15 or 75/25, here I choose 75/25 via “test_size”.\n",
    "X is the bag of words, y is 0 or 1 (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into \n",
    "# the Training set and Test set \n",
    "from sklearn.model_selection import train_test_split   \n",
    "# experiment with \"test_size\" \n",
    "# to get better results \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Fitting a Predictive Model (here random forest)\n",
    "Since Random fored is ensemble model (made of many trees) from sklearn.ensemble, import RandomForestClassifier class\n",
    "\n",
    "\n",
    "With 501 tree or “n_estimators” and criterion as ‘entropy’\n",
    "\n",
    "\n",
    "Fit the model via .fit() method with attributes X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=501, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Random Forest Classification \n",
    "# to the Training set \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "  \n",
    "# n_estimators can be said as number of \n",
    "# trees, experiment with n_estimators \n",
    "# to get better results  \n",
    "model = RandomForestClassifier(n_estimators = 501, criterion = 'entropy')                               \n",
    "model.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Pridicting Final Results \n",
    "via using .predict() method with attribute X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results \n",
    "y_pred = model.predict(X_test) \n",
    "  \n",
    "y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: To know the accuracy, confusion matrix is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[114  21]\n",
      " [ 41  74]]\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix \n",
    "from sklearn.metrics import confusion_matrix   \n",
    "cm = confusion_matrix(y_test, y_pred)   \n",
    "print(cm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
